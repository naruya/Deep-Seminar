{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sjv5haiCBqU1"
   },
   "source": [
    "# 誤差逆伝播と自動微分\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7baJia9AMCor"
   },
   "source": [
    "# 自動微分\n",
    "\n",
    "まずは触ってみましょう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BjATA_TQrl0h"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_iyKsSvHiEG"
   },
   "source": [
    "1次関数の微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cvY5Ix3nDET2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([2.]),)\n",
      "(tensor([2.]),)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "y = x*2\n",
    "print(torch.autograd.grad(y, x))  # dy/dx|x=1 = 2 (1次関数なのでxの値に関係ない)\n",
    "\n",
    "x = torch.tensor([3.], requires_grad=True)\n",
    "\n",
    "y = x*2\n",
    "print(torch.autograd.grad(y, x))  # dy/dx|x=3 = ?? (1次関数なのでxの値に関係ない)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkwpiblfHkwn"
   },
   "source": [
    "高次関数の微分\n",
    "- 深層学習で二乗はたまに出現します(誤差関数など)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "88W67VyGDKfF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([6.]),)\n",
      "(tensor([27.]),)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.], requires_grad=True)\n",
    "\n",
    "y = x**2\n",
    "print(torch.autograd.grad(y, x))  # dy/dx|x=3 = ??\n",
    "\n",
    "y = x**3\n",
    "print(torch.autograd.grad(y, x))  # dy/dx|x=3 = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "J_baWYQAD3wC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([8.]),)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.], requires_grad=True)\n",
    "\n",
    "y = x**2 + x*2 + 2\n",
    "print(torch.autograd.grad(y, x))  # dy/dx|x=3 = ??\n",
    "\n",
    "# メモリ使用量を抑えるため一度微分されるとyと他の変数の関係は捨てられることに注意\n",
    "# print(torch.autograd.grad(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXlLevGDIAEB"
   },
   "source": [
    "二階微分\n",
    "- 深層学習では普通は使わないですが使うときもあります\n",
    "  - [\\[DL輪読会\\] DeepLearningと曲がったパラメータ空間](https://www.slideshare.net/DeepLearningJP2016/dldeeplearning-scalable-trustregion-method-for-deep-reinforcement-learning-using-kroneckerfactored-approximation)\n",
    "  - [\\[DL輪読会\\]1次近似系MAMLとその理論的背景](https://www.slideshare.net/DeepLearningJP2016/dl1maml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WmtGS-F1E0P_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([17.], grad_fn=<AddBackward0>),)\n",
      "(tensor([14.], grad_fn=<AddBackward0>),)\n",
      "(tensor([6.]),)\n"
     ]
    }
   ],
   "source": [
    "# create_graphをTrueにしないと他の変数との関係が保存されず微分できないので注意\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "y = x**3 + x**2 + x + 1\n",
    "\n",
    "dy = torch.autograd.grad(y, x, create_graph=True)  # 3x**2 + 2x + 1 | x=2\n",
    "print(dy)\n",
    "\n",
    "ddy = torch.autograd.grad(dy[0], x, create_graph=True)  # ??\n",
    "print(ddy)\n",
    "\n",
    "dddy = torch.autograd.grad(ddy[0], x)  # ??\n",
    "print(dddy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsyqfQ6UKvhZ"
   },
   "source": [
    "適当な関数も微分してみましょう\n",
    "\n",
    "- いろんな関数が実装されています: https://pytorch.org/docs/stable/torch.html#math-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oc_mnxJQJyXS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp\n",
      "tensor([2.7183], grad_fn=<ExpBackward>)\n",
      "(tensor([2.7183], grad_fn=<MulBackward0>),)\n",
      "(tensor([2.7183], grad_fn=<MulBackward0>),)\n",
      "(tensor([2.7183]),)\n",
      "\n",
      "sin\n",
      "tensor([0.0006], grad_fn=<SinBackward>)\n",
      "(tensor([-1.0000], grad_fn=<MulBackward0>),)\n",
      "(tensor([-0.0006], grad_fn=<MulBackward0>),)\n",
      "(tensor([1.0000]),)\n"
     ]
    }
   ],
   "source": [
    "print(\"exp\")\n",
    "x = torch.tensor([1.], requires_grad=True)\n",
    "y = torch.exp(x)                                        # e^x\n",
    "dy = torch.autograd.grad(y, x, create_graph=True)       # e^x\n",
    "ddy = torch.autograd.grad(dy[0], x, create_graph=True)  # e^x\n",
    "dddy = torch.autograd.grad(ddy[0], x)                   # e^x\n",
    "print(y); print(dy); print(ddy); print(dddy)\n",
    "\n",
    "print(\"\\nsin\")\n",
    "x = torch.tensor([3.141], requires_grad=True)\n",
    "y = torch.sin(x)                                        # sin(x)\n",
    "dy = torch.autograd.grad(y, x, create_graph=True)       # cos(x)\n",
    "ddy = torch.autograd.grad(dy[0], x, create_graph=True)  # -sin(x)\n",
    "dddy = torch.autograd.grad(ddy[0], x)                   # -cos(x)\n",
    "print(y); print(dy); print(ddy); print(dddy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFP_TXuaWPaD"
   },
   "source": [
    "# 逆伝播\n",
    "\n",
    "- 逆伝播によって、出力yを大きくするために重みを動かすべき方向が分かります。\n",
    "- 具体的には合成関数の微分 (**連鎖律**) が使われています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jENfpGNqVQZh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-9.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.], requires_grad=True)\n",
    "\n",
    "w1 = torch.tensor([2.], requires_grad=True)\n",
    "b1 = torch.tensor([-10.], requires_grad=True)\n",
    "\n",
    "w2 = torch.tensor([4.], requires_grad=True)\n",
    "b2 = torch.tensor([7.], requires_grad=True)\n",
    "\n",
    "h1 = w1*x+b1  # -4\n",
    "y = w2*h1+b2  # -9\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FrqkKBNjVn33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([12.]),)\n",
      "(tensor([4.]),)\n",
      "(tensor([-4.]),)\n",
      "(tensor([1.]),)\n"
     ]
    }
   ],
   "source": [
    "# yを大きくするには...？ (大きく==正の方向に動かす)\n",
    "print(torch.autograd.grad(y, w1, retain_graph=True))  # 増やせ\n",
    "print(torch.autograd.grad(y, b1, retain_graph=True))  # 増やせ\n",
    "print(torch.autograd.grad(y, w2, retain_graph=True))  # ??\n",
    "print(torch.autograd.grad(y, b2, retain_graph=True))  # ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zXUoQH4yXp_5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n",
      "tensor([4.])\n",
      "tensor([-4.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# 一発で全部のパラメータに関して微分してくれます。便利！\n",
    "y.backward(retain_graph=True)\n",
    "\n",
    "# backwardを使うと、各パラメータに勾配情報.gradがセットされる\n",
    "# backwardを二回以上実行するとgradが加算されます\n",
    "print(w1.grad); print(b1.grad); print(w2.grad); print(b2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfnM4SuxLkCx"
   },
   "source": [
    "# 誤差逆伝播\n",
    "\n",
    "- lossに対して連鎖律を適用するとlossを小さくするような変数の動かし方がわかります。(**誤差逆伝播**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0g6XXlF5LF0A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.], requires_grad=True)\n",
      "tensor([1.], requires_grad=True)\n",
      "pred = 16.0, true = 10.0, loss = 36.0\n",
      "tensor([36.])\n",
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = torch.tensor([3.])\n",
    "true = torch.tensor([10.])\n",
    "\n",
    "w = torch.tensor([5.], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "print(w); print(b)\n",
    "\n",
    "pred = w*x+b\n",
    "loss = torch.pow(pred - true, 2)\n",
    "\n",
    "print(\"pred = {}, true = {}, loss = {}\".format(pred.item(), true.item(), loss.item()))\n",
    "\n",
    "loss.backward()\n",
    "print(w.grad); print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3QVgJrBfNOb"
   },
   "source": [
    "- lossを減らす方向が分かったので、wとbを更新してみましょう。\n",
    "- 勾配が正なら負の方向に、勾配が負なら正の方向に、勾配×**学習率**の値だけ動かします。(これを**勾配降下**といいます。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "O9oQ1H3OfS5n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.6400], requires_grad=True)\n",
      "tensor([0.8800], requires_grad=True)\n",
      "pred = 14.800000190734863, true = 10.0, loss = 23.040000915527344\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01  # learning_rate, 学習率\n",
    "\n",
    "w = torch.tensor([w.detach() - lr * w.grad], requires_grad=True)\n",
    "b = torch.tensor([b.detach() - lr * b.grad], requires_grad=True)\n",
    "\n",
    "print(w); print(b)\n",
    "\n",
    "pred = w*x+b\n",
    "loss = torch.pow(pred - true, 2)\n",
    "\n",
    "print(\"pred = {}, true = {}, loss = {}\".format(pred.item(), true.item(), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6O1bBttlZ4y"
   },
   "source": [
    "- さらに更新してみましょう。誤差が小さくなっていることが確認できると思います。\n",
    "  - うまく行かない場合は学習率が大きすぎる可能性が高いです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7SxahJVtfLuU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred = 13.84000015258789, true = 10.0, loss = 14.745600700378418\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "w = torch.tensor([w.detach() - lr * w.grad], requires_grad=True)\n",
    "b = torch.tensor([b.detach() - lr * b.grad], requires_grad=True)\n",
    "\n",
    "pred = w*x+b\n",
    "loss = torch.pow(pred - true, 2)\n",
    "\n",
    "print(\"pred = {}, true = {}, loss = {}\".format(pred.item(), true.item(), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaEtiFFoTTlT"
   },
   "source": [
    "<img src=\"https://tutorials.chainer.org/ja/_images/13_13.png\" width=\"800\">\n",
    "\n",
    "出典: https://tutorials.chainer.org/ja/13_Basics_of_Neural_Networks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q01Rx6xgk5CU"
   },
   "source": [
    "# 最適化\n",
    "\n",
    "- 変数を自動で更新する実装も備わっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TEyky2RMlL6y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.], requires_grad=True)\n",
      "tensor([1.], requires_grad=True)\n",
      "pred = 16.0, true = 10.0, loss = 36.0\n",
      "------------------------------------------------------------\n",
      "tensor([4.6400], requires_grad=True)\n",
      "tensor([0.8800], requires_grad=True)\n",
      "pred = 14.800000190734863, true = 10.0, loss = 23.040000915527344\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import optim\n",
    "\n",
    "x = torch.tensor([3.])\n",
    "true = torch.tensor([10.])\n",
    "\n",
    "w = torch.tensor([5.], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "print(w); print(b)\n",
    "optimizer = optim.SGD((w, b), lr=0.01, momentum=0)\n",
    "\n",
    "pred = w*x+b \n",
    "loss = torch.pow(pred - true, 2)\n",
    "\n",
    "print(\"pred = {}, true = {}, loss = {}\".format(pred.item(), true.item(), loss.item()))\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(w); print(b)\n",
    "\n",
    "pred = w*x+b\n",
    "loss = torch.pow(pred - true, 2)\n",
    "print(\"pred = {}, true = {}, loss = {}\".format(pred.item(), true.item(), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ql7r6llqzf1H"
   },
   "outputs": [],
   "source": [
    "del w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqEP5U7KpzIW"
   },
   "source": [
    "- 変数をまとめてクラスに、最適化を関数にして複数回変数を更新しましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IxthsznppybN"
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "  def __init__(self):\n",
    "    self.w = torch.tensor([5.], requires_grad=True)\n",
    "    self.b = torch.tensor([1.], requires_grad=True)\n",
    "    self.params = (self.w, self.b)\n",
    "\n",
    "  def forward(self, x):\n",
    "    pred = self.w*x+self.b\n",
    "    return pred\n",
    "\n",
    "def train():\n",
    "  x, true = data  # データを取ってくる(今回データは1つしかないですが)\n",
    "  pred = model.forward(x)\n",
    "  loss = torch.pow(pred- true, 2)\n",
    "\n",
    "  print(\"pred = {}, true = {}, loss = {}\".format(pred.item(), true.item(), loss.item()))\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  # 誤差逆伝播\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  loss_hist.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "A6C5DNmjs77p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred = 16.0, true = 10.0, loss = 36.0\n",
      "pred = 14.800000190734863, true = 10.0, loss = 23.040000915527344\n",
      "pred = 13.84000015258789, true = 10.0, loss = 14.745600700378418\n",
      "pred = 13.071999549865723, true = 10.0, loss = 9.43718147277832\n",
      "pred = 12.457598686218262, true = 10.0, loss = 6.039791107177734\n",
      "pred = 11.966079711914062, true = 10.0, loss = 3.865469455718994\n",
      "pred = 11.572863578796387, true = 10.0, loss = 2.4738998413085938\n",
      "pred = 11.258291244506836, true = 10.0, loss = 1.5832968950271606\n",
      "pred = 11.006633758544922, true = 10.0, loss = 1.013311505317688\n",
      "pred = 10.805306434631348, true = 10.0, loss = 0.648518443107605\n",
      "pred = 10.644245147705078, true = 10.0, loss = 0.41505181789398193\n",
      "pred = 10.515396118164062, true = 10.0, loss = 0.26563316583633423\n",
      "pred = 10.41231632232666, true = 10.0, loss = 0.17000475525856018\n",
      "pred = 10.329852104187012, true = 10.0, loss = 0.10880240797996521\n",
      "pred = 10.263882637023926, true = 10.0, loss = 0.06963404268026352\n",
      "pred = 10.211106300354004, true = 10.0, loss = 0.044565871357917786\n",
      "pred = 10.168883323669434, true = 10.0, loss = 0.028521576896309853\n",
      "pred = 10.13510799407959, true = 10.0, loss = 0.018254170194268227\n",
      "pred = 10.108085632324219, true = 10.0, loss = 0.01168250385671854\n",
      "pred = 10.086468696594238, true = 10.0, loss = 0.007476835511624813\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.])\n",
    "true = torch.tensor([10.])\n",
    "\n",
    "data = (x, true)\n",
    "\n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.params, lr=0.01, momentum=0)\n",
    "\n",
    "loss_hist = []\n",
    "\n",
    "for epoch in range(20):\n",
    "  train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lQDv79o4s98Z"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 学習が進んでいる様子が確認できます\n",
    "plt.plot(loss_hist); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BSVL-UEuWwG"
   },
   "source": [
    "前章ではnumpyでモデルを定義しましたが、今回のように深層学習ライブラリを使ってモデルを定義することで簡単に自動微分でき学習を行うことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oLJvvu7wrxL"
   },
   "source": [
    "前章で扱ったモデルの定義と今回扱ったtrain関数とで、  \n",
    "深層学習を使う上で必要な基本的なコーディングがほぼ全てできるようになりました。\n",
    "\n",
    "ここまでしっかり理解すれば、深層学習一応わかります！と言っていいと思います。(ただしTPOによる)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIOs5DPE2y8n"
   },
   "source": [
    "# 演習 (余力がある人向け)\n",
    "\n",
    "- 白紙の状態から下の図が書けて誤差の各パラメータでの微分が求められるように練習すると、かなり理解が深まると思います。(激しく推奨)\n",
    "- より深く基礎から深層学習を理解したい人は、リンク先のchainerのチュートリアルも非常に参考になると思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njH9ZKkw2og7"
   },
   "source": [
    "<img src=\"https://tutorials.chainer.org/ja/_images/13_backpropagation.gif\" width=\"760\">\n",
    "\n",
    "出典: https://tutorials.chainer.org/ja/13_Basics_of_Neural_Networks.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-E6cuTQetCqL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPKVRV9hbQsSRUUB25LTkWF",
   "collapsed_sections": [],
   "name": "exercise3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
